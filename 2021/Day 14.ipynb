{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polymers\n",
    "\n",
    "This looks nasty, because the most obvious solution here is to build a map function that takes each pairwise letter and produces a new combination, and then combines all the resultant combinations, but I have a feeling that this is going to expand into memory hogging space really quickly.  There's something about the question here that makes me wonder if there's an easier way to calculate teh totals, but I can't think of it yet.\n",
    "\n",
    "So here we go with the version that is going to grow a lot over just 10 times.\n",
    "\n",
    "Start with a string, and use a lookup table.\n",
    "\n",
    "The example lookup table says something like \"CN->C\", but I suspect that we should think about the substitutions we want to make, so CN->CCN.  That's a complete substitution.\n",
    "\n",
    "However, as we run over something like NNCB, if we turn NN->NCN and NC->NBC and CB->CHB then we're going to end up with NCNNBCCHB with duplicated letters.  Instead if we crop the first letter, we end up with NN->CN, NC->BC and CB->HB, we end up with \"N\" plus CNBCHB.\n",
    "\n",
    "That's a much simpler mapping, and we can turn the initial table into that lookup form fairly easily I hope.  Let's give that a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import ipytest and get it setup for use in Python Notebook\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lines = \"\"\"NNCB\n",
    "\n",
    "CH -> B\n",
    "HH -> N\n",
    "CB -> H\n",
    "NH -> C\n",
    "HB -> C\n",
    "HC -> B\n",
    "HN -> C\n",
    "NN -> C\n",
    "BH -> H\n",
    "NC -> B\n",
    "NB -> B\n",
    "BN -> B\n",
    "BB -> N\n",
    "BC -> B\n",
    "CC -> N\n",
    "CN -> C\"\"\".split(\"\\n\")\n",
    "\n",
    "def parse_lines(lines):\n",
    "    template = lines[0]\n",
    "    mapping = {}\n",
    "    for line in lines[2:]:\n",
    "        start, append = line.split(\" -> \")\n",
    "        mapping[start] = append+start[1]\n",
    "    return template, mapping\n",
    "\n",
    "\n",
    "template, mapping = parse_lines(test_lines)\n",
    "assert template == \"NNCB\"\n",
    "assert mapping[\"CH\"] == \"BH\"\n",
    "assert mapping[\"HH\"] == \"NH\"\n",
    "assert mapping[\"CB\"] == \"HB\"\n",
    "assert mapping[\"NH\"] == \"CH\"\n",
    "assert len(mapping) == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try some expansion, so each iteration is equal to the pairwise mapping of letters, looked up in the substituion chart.  Luckily python has us already covered for pairwise iteration of a collection, in itertools with the well named \"pairwise\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('B', 1749) ('H', 161)\n",
      "1588\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import collections\n",
    "\n",
    "def iterate(polymer, mapping):\n",
    "    l = [polymer[0]]\n",
    "    for pair in itertools.pairwise(polymer):\n",
    "        l.append(mapping[\"\".join(pair)])\n",
    "    return \"\".join(l)\n",
    "\n",
    "assert iterate(template, mapping) == \"NCNBCHB\"\n",
    "assert iterate(\"NCNBCHB\", mapping) == \"NBCCNBBBCBHCB\"\n",
    "assert iterate(\"NBCCNBBBCBHCB\", mapping) == \"NBBBCNCCNBBNBNBBCHBHHBCHB\"\n",
    "assert iterate(\"NBBBCNCCNBBNBNBBCHBHHBCHB\", mapping) == \"NBBNBNBBCCNBCNCCNBBNBBNBBBNBBNBBCBHCBHHNHCBBCBHCB\"\n",
    "\n",
    "polymer = template\n",
    "for i in range(10):\n",
    "    polymer = iterate(polymer, mapping)\n",
    "\n",
    "assert 3073 == len(polymer)\n",
    "counts = collections.Counter(polymer).most_common()\n",
    "print(counts[0],counts[-1])\n",
    "print(counts[0][1]-counts[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do that on production data... gulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19457\n",
      "('B', 3330) ('H', 533)\n",
      "2797\n"
     ]
    }
   ],
   "source": [
    "polymer, mapping = parse_lines([line.strip() for line in open(\"day14.txt\").readlines()])\n",
    "for i in range(10):\n",
    "    polymer = iterate(polymer, mapping)\n",
    "\n",
    "print(len(polymer))\n",
    "counts = collections.Counter(polymer).most_common()\n",
    "print(counts[0],counts[-1])\n",
    "print(counts[0][1]-counts[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More steps\n",
    "\n",
    "As I thought, 30 more steps.  I suspect this doubles in complexity with each step taken.\n",
    "\n",
    "More importantly, if there are in fact around 2 trillion \"B\"'s in the string, then we're going to need around 2TB of ram to just hold the \"B\"'s in bytes, so that's not going to work.\n",
    "\n",
    "We're going to need a different data structure and different algorithm, and I've not had the week to think about that I'm afraid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
